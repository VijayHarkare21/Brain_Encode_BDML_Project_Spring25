{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90120c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel, ViTImageProcessor, ViTModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "import pickle\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a2fc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f68668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "METADATA_PATH = r'/scratch/vjh9526/bdml_2025/project/datasets/THINGS-EEG/image_set/image_metadata.npy'\n",
    "TRAIN_DIR = r'/scratch/vjh9526/bdml_2025/project/datasets/THINGS-EEG/image_set/training_images'\n",
    "TEST_DIR = r'/scratch/vjh9526/bdml_2025/project/datasets/THINGS-EEG/image_set/test_images'\n",
    "RESULTS_PATH = r'/scratch/vjh9526/bdml_2025/project/code/classifiers/natural_modes/results_new.pkl'\n",
    "FEATURES_DIR = r'/scratch/vjh9526/bdml_2025/project/code/classifiers/natural_modes/features'\n",
    "THINGS_MAP_PATH = r\"/scratch/vjh9526/bdml_2025/project/code/classifiers/natural_modes/category27_top-down.tsv\"\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8de57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading image labels from /scratch/vjh9526/bdml_2025/project/datasets/THINGS-EEG/image_set/image_metadata.npy\n",
      "[INFO] Loading high-level image labels from /scratch/vjh9526/bdml_2025/project/code/classifiers/natural_modes/category27_top-down.tsv\n"
     ]
    }
   ],
   "source": [
    "# your existing function\n",
    "def load_image_labels(metadata_path: str, things_map_path: str):\n",
    "    \"\"\"Load image labels from metadata file and map to high-level THINGS concepts.\"\"\"\n",
    "    print(f\"[INFO] Loading image labels from {metadata_path}\")\n",
    "    print(f\"[INFO] Loading high-level image labels from {things_map_path}\")\n",
    "    meta = np.load(metadata_path, allow_pickle=True).item()\n",
    "    things_map = pd.read_csv(things_map_path, delimiter=\"\\t\")\n",
    "    files = meta['train_img_files']\n",
    "    concepts = meta['train_img_concepts']\n",
    "    things_concepts = meta['train_img_concepts_THINGS']\n",
    "    \n",
    "    path_to_label = {}\n",
    "    for things_concept, concept, fname in zip(things_concepts, concepts, files):\n",
    "        idx = int(things_concept.split(\"_\")[0]) - 1\n",
    "        row = things_map.iloc[idx]\n",
    "        # if any 1 in row, pick its column name; else 'miscellaneous'\n",
    "        if (row == 0).all():\n",
    "            high_concept = 'miscellaneous'\n",
    "        else:\n",
    "            high_concept = str(things_map.columns[row == 1][0])\n",
    "        path_key = os.path.join(concept, fname)\n",
    "        path_to_label[path_key] = high_concept\n",
    "        \n",
    "    return path_to_label\n",
    "\n",
    "\n",
    "# 2) Build high-level labels mapping\n",
    "path_to_label = load_image_labels(METADATA_PATH, THINGS_MAP_PATH)\n",
    "\n",
    "# 3) Extract raw file lists & fine-grained concepts\n",
    "meta      = np.load(METADATA_PATH, allow_pickle=True).item()\n",
    "files     = meta['train_img_files']        # e.g. [\"img1.png\", \"img2.png\", …]\n",
    "concepts  = meta['train_img_concepts']     # e.g. [\"cat\", \"dog\", …]\n",
    "\n",
    "# 4) Build the list of high-level labels\n",
    "labels_high = [\n",
    "    path_to_label[os.path.join(concept, fname)]\n",
    "    for concept, fname in zip(concepts, files)\n",
    "]\n",
    "\n",
    "# 5) Encode high-level labels to integers\n",
    "unique_high = sorted(set(labels_high))\n",
    "high_to_idx = {lbl: i for i, lbl in enumerate(unique_high)}\n",
    "y_high = np.array([high_to_idx[lbl] for lbl in labels_high])\n",
    "\n",
    "# 6) Stratified train/val split on high-level labels\n",
    "train_files, val_files, train_concepts, val_concepts, y_train, y_val = train_test_split(\n",
    "    files, concepts, y_high,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_high\n",
    ")\n",
    "\n",
    "# 7) Transforms (unchanged)\n",
    "def get_transforms():\n",
    "    train_tfm = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_tfm = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    return train_tfm, val_tfm\n",
    "\n",
    "train_tfm, val_tfm = get_transforms()\n",
    "\n",
    "# 8) Dataset & DataLoaders (now using y_train, y_val of high-level labels)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, base_dir, concepts, files, labels, transform):\n",
    "        self.base_dir = base_dir\n",
    "        self.concepts = concepts\n",
    "        self.files    = files\n",
    "        self.labels   = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        concept = self.concepts[idx]\n",
    "        fname   = self.files[idx]\n",
    "        path    = os.path.join(self.base_dir, concept, fname)\n",
    "        img     = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.labels[idx]\n",
    "\n",
    "train_dataset = ImageDataset(\n",
    "    TRAIN_DIR, train_concepts, train_files, y_train, train_tfm\n",
    ")\n",
    "val_dataset = ImageDataset(\n",
    "    TRAIN_DIR, val_concepts, val_files, y_val, val_tfm\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=True, num_workers=4\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, batch_size=BATCH_SIZE,\n",
    "    shuffle=False, num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "221e5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- Extractor Definitions -------------------\n",
    "extractor_configs = [\n",
    "    ('resnet50', lambda: models.resnet50(pretrained=True).eval().to(device), None),\n",
    "    # ('vgg19',   lambda: (lambda m: setattr(m,'classifier',torch.nn.Sequential(*list(m.classifier.children())[:-3])) or m)(models.vgg19(pretrained=True)).eval().to(device), None),\n",
    "    ('clip',    None, (CLIPProcessor.from_pretrained, CLIPModel.from_pretrained, 'openai/clip-vit-base-patch32')),\n",
    "    ('vit',     None, (ViTImageProcessor.from_pretrained, ViTModel.from_pretrained, 'google/vit-base-patch16-224-in21k'))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71069bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extracting features with resnet50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resnet50-train] batch 1/52 in 6.87s\n",
      "[resnet50-train] batch 2/52 in 5.26s\n",
      "[resnet50-train] batch 3/52 in 5.31s\n",
      "[resnet50-train] batch 4/52 in 5.01s\n",
      "[resnet50-train] batch 5/52 in 4.30s\n",
      "[resnet50-train] batch 6/52 in 4.08s\n",
      "[resnet50-train] batch 7/52 in 3.71s\n",
      "[resnet50-train] batch 8/52 in 4.06s\n",
      "[resnet50-train] batch 9/52 in 3.95s\n",
      "[resnet50-train] batch 10/52 in 3.97s\n",
      "[resnet50-train] batch 11/52 in 3.63s\n",
      "[resnet50-train] batch 12/52 in 3.90s\n",
      "[resnet50-train] batch 13/52 in 3.70s\n",
      "[resnet50-train] batch 14/52 in 3.95s\n",
      "[resnet50-train] batch 15/52 in 3.59s\n",
      "[resnet50-train] batch 16/52 in 3.96s\n",
      "[resnet50-train] batch 17/52 in 4.05s\n",
      "[resnet50-train] batch 18/52 in 3.82s\n",
      "[resnet50-train] batch 19/52 in 3.72s\n",
      "[resnet50-train] batch 20/52 in 3.85s\n",
      "[resnet50-train] batch 21/52 in 4.05s\n",
      "[resnet50-train] batch 22/52 in 4.05s\n",
      "[resnet50-train] batch 23/52 in 4.00s\n",
      "[resnet50-train] batch 24/52 in 3.85s\n",
      "[resnet50-train] batch 25/52 in 3.12s\n",
      "[resnet50-train] batch 26/52 in 3.06s\n",
      "[resnet50-train] batch 27/52 in 2.96s\n",
      "[resnet50-train] batch 28/52 in 3.65s\n",
      "[resnet50-train] batch 29/52 in 4.30s\n",
      "[resnet50-train] batch 30/52 in 3.88s\n",
      "[resnet50-train] batch 31/52 in 3.93s\n",
      "[resnet50-train] batch 32/52 in 3.77s\n",
      "[resnet50-train] batch 33/52 in 3.98s\n",
      "[resnet50-train] batch 34/52 in 3.84s\n",
      "[resnet50-train] batch 35/52 in 3.78s\n",
      "[resnet50-train] batch 36/52 in 3.87s\n",
      "[resnet50-train] batch 37/52 in 3.90s\n",
      "[resnet50-train] batch 38/52 in 3.59s\n",
      "[resnet50-train] batch 39/52 in 3.87s\n",
      "[resnet50-train] batch 40/52 in 4.09s\n",
      "[resnet50-train] batch 41/52 in 3.93s\n",
      "[resnet50-train] batch 42/52 in 3.63s\n",
      "[resnet50-train] batch 43/52 in 3.77s\n",
      "[resnet50-train] batch 44/52 in 4.17s\n",
      "[resnet50-train] batch 45/52 in 2.08s\n",
      "[resnet50-train] batch 46/52 in 1.92s\n",
      "[resnet50-train] batch 47/52 in 1.92s\n",
      "[resnet50-train] batch 48/52 in 1.92s\n",
      "[resnet50-train] batch 49/52 in 1.91s\n",
      "[resnet50-train] batch 50/52 in 1.92s\n",
      "[resnet50-train] batch 51/52 in 1.92s\n",
      "[resnet50-train] batch 52/52 in 1.45s\n",
      "[resnet50-val] batch 1/13 in 5.11s\n",
      "[resnet50-val] batch 2/13 in 5.67s\n",
      "[resnet50-val] batch 3/13 in 5.24s\n",
      "[resnet50-val] batch 4/13 in 3.84s\n",
      "[resnet50-val] batch 5/13 in 3.96s\n",
      "[resnet50-val] batch 6/13 in 2.36s\n",
      "[resnet50-val] batch 7/13 in 2.37s\n",
      "[resnet50-val] batch 8/13 in 2.33s\n",
      "[resnet50-val] batch 9/13 in 1.86s\n",
      "[resnet50-val] batch 10/13 in 1.85s\n",
      "[resnet50-val] batch 11/13 in 1.85s\n",
      "[resnet50-val] batch 12/13 in 1.85s\n",
      "[resnet50-val] batch 13/13 in 1.73s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished resnet50, time: 253.72s\n",
      "\n",
      "--- Extracting features with clip ---\n",
      "[clip-train] batch 1/52 in 3.25s\n",
      "[clip-train] batch 2/52 in 3.87s\n",
      "[clip-train] batch 3/52 in 3.19s\n",
      "[clip-train] batch 4/52 in 3.85s\n",
      "[clip-train] batch 5/52 in 2.32s\n",
      "[clip-train] batch 6/52 in 2.78s\n",
      "[clip-train] batch 7/52 in 3.30s\n",
      "[clip-train] batch 8/52 in 3.96s\n",
      "[clip-train] batch 9/52 in 3.27s\n",
      "[clip-train] batch 10/52 in 3.44s\n",
      "[clip-train] batch 11/52 in 2.76s\n",
      "[clip-train] batch 12/52 in 2.68s\n",
      "[clip-train] batch 13/52 in 3.10s\n",
      "[clip-train] batch 14/52 in 3.43s\n",
      "[clip-train] batch 15/52 in 3.83s\n",
      "[clip-train] batch 16/52 in 2.68s\n",
      "[clip-train] batch 17/52 in 3.62s\n",
      "[clip-train] batch 18/52 in 3.09s\n",
      "[clip-train] batch 19/52 in 2.42s\n",
      "[clip-train] batch 20/52 in 3.46s\n",
      "[clip-train] batch 21/52 in 2.60s\n",
      "[clip-train] batch 22/52 in 3.02s\n",
      "[clip-train] batch 23/52 in 2.92s\n",
      "[clip-train] batch 24/52 in 2.50s\n",
      "[clip-train] batch 25/52 in 3.02s\n",
      "[clip-train] batch 26/52 in 3.40s\n",
      "[clip-train] batch 27/52 in 3.17s\n",
      "[clip-train] batch 28/52 in 3.02s\n",
      "[clip-train] batch 29/52 in 2.88s\n",
      "[clip-train] batch 30/52 in 2.22s\n",
      "[clip-train] batch 31/52 in 2.21s\n",
      "[clip-train] batch 32/52 in 3.55s\n",
      "[clip-train] batch 33/52 in 3.49s\n",
      "[clip-train] batch 34/52 in 2.89s\n",
      "[clip-train] batch 35/52 in 3.56s\n",
      "[clip-train] batch 36/52 in 3.34s\n",
      "[clip-train] batch 37/52 in 3.49s\n",
      "[clip-train] batch 38/52 in 3.38s\n",
      "[clip-train] batch 39/52 in 3.12s\n",
      "[clip-train] batch 40/52 in 3.35s\n",
      "[clip-train] batch 41/52 in 2.33s\n",
      "[clip-train] batch 42/52 in 2.60s\n",
      "[clip-train] batch 43/52 in 3.41s\n",
      "[clip-train] batch 44/52 in 2.96s\n",
      "[clip-train] batch 45/52 in 1.96s\n",
      "[clip-train] batch 46/52 in 1.52s\n",
      "[clip-train] batch 47/52 in 2.32s\n",
      "[clip-train] batch 48/52 in 1.81s\n",
      "[clip-train] batch 49/52 in 1.52s\n",
      "[clip-train] batch 50/52 in 1.52s\n",
      "[clip-train] batch 51/52 in 1.57s\n",
      "[clip-train] batch 52/52 in 1.07s\n",
      "[clip-val] batch 1/13 in 4.57s\n",
      "[clip-val] batch 2/13 in 3.03s\n",
      "[clip-val] batch 3/13 in 3.35s\n",
      "[clip-val] batch 4/13 in 3.10s\n",
      "[clip-val] batch 5/13 in 2.07s\n",
      "[clip-val] batch 6/13 in 1.55s\n",
      "[clip-val] batch 7/13 in 1.57s\n",
      "[clip-val] batch 8/13 in 1.53s\n",
      "[clip-val] batch 9/13 in 1.54s\n",
      "[clip-val] batch 10/13 in 1.55s\n",
      "[clip-val] batch 11/13 in 1.54s\n",
      "[clip-val] batch 12/13 in 1.53s\n",
      "[clip-val] batch 13/13 in 1.45s\n",
      "Finished clip, time: 201.45s\n",
      "\n",
      "--- Extracting features with vit ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/transformers/image_processing_utils.py:42: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vit-train] batch 1/52 in 10.82s\n",
      "[vit-train] batch 2/52 in 7.40s\n",
      "[vit-train] batch 3/52 in 5.94s\n",
      "[vit-train] batch 4/52 in 6.19s\n",
      "[vit-train] batch 5/52 in 5.59s\n",
      "[vit-train] batch 6/52 in 5.98s\n",
      "[vit-train] batch 7/52 in 5.41s\n",
      "[vit-train] batch 8/52 in 5.48s\n",
      "[vit-train] batch 9/52 in 5.67s\n",
      "[vit-train] batch 10/52 in 5.78s\n",
      "[vit-train] batch 11/52 in 5.78s\n",
      "[vit-train] batch 12/52 in 6.51s\n",
      "[vit-train] batch 13/52 in 5.76s\n",
      "[vit-train] batch 14/52 in 5.66s\n",
      "[vit-train] batch 15/52 in 5.87s\n",
      "[vit-train] batch 16/52 in 5.66s\n",
      "[vit-train] batch 17/52 in 5.70s\n",
      "[vit-train] batch 18/52 in 5.79s\n",
      "[vit-train] batch 19/52 in 5.75s\n",
      "[vit-train] batch 20/52 in 5.55s\n",
      "[vit-train] batch 21/52 in 5.78s\n",
      "[vit-train] batch 22/52 in 5.42s\n",
      "[vit-train] batch 23/52 in 5.55s\n",
      "[vit-train] batch 24/52 in 5.69s\n",
      "[vit-train] batch 25/52 in 6.40s\n",
      "[vit-train] batch 26/52 in 5.91s\n",
      "[vit-train] batch 27/52 in 5.59s\n",
      "[vit-train] batch 28/52 in 5.91s\n",
      "[vit-train] batch 29/52 in 5.85s\n",
      "[vit-train] batch 30/52 in 6.77s\n",
      "[vit-train] batch 31/52 in 5.53s\n",
      "[vit-train] batch 32/52 in 5.80s\n",
      "[vit-train] batch 33/52 in 5.85s\n",
      "[vit-train] batch 34/52 in 5.49s\n",
      "[vit-train] batch 35/52 in 5.90s\n",
      "[vit-train] batch 36/52 in 5.50s\n",
      "[vit-train] batch 37/52 in 5.67s\n",
      "[vit-train] batch 38/52 in 5.57s\n",
      "[vit-train] batch 39/52 in 6.81s\n",
      "[vit-train] batch 40/52 in 6.48s\n",
      "[vit-train] batch 41/52 in 6.68s\n",
      "[vit-train] batch 42/52 in 6.56s\n",
      "[vit-train] batch 43/52 in 6.08s\n",
      "[vit-train] batch 44/52 in 5.41s\n",
      "[vit-train] batch 45/52 in 6.77s\n",
      "[vit-train] batch 46/52 in 4.75s\n",
      "[vit-train] batch 47/52 in 4.51s\n",
      "[vit-train] batch 48/52 in 4.56s\n",
      "[vit-train] batch 49/52 in 4.70s\n",
      "[vit-train] batch 50/52 in 4.19s\n",
      "[vit-train] batch 51/52 in 4.15s\n",
      "[vit-train] batch 52/52 in 2.82s\n",
      "[vit-val] batch 1/13 in 9.67s\n",
      "[vit-val] batch 2/13 in 5.00s\n",
      "[vit-val] batch 3/13 in 5.08s\n",
      "[vit-val] batch 4/13 in 6.22s\n",
      "[vit-val] batch 5/13 in 5.06s\n",
      "[vit-val] batch 6/13 in 4.65s\n",
      "[vit-val] batch 7/13 in 4.13s\n",
      "[vit-val] batch 8/13 in 4.11s\n",
      "[vit-val] batch 9/13 in 4.07s\n",
      "[vit-val] batch 10/13 in 4.08s\n",
      "[vit-val] batch 11/13 in 4.05s\n",
      "[vit-val] batch 12/13 in 4.09s\n",
      "[vit-val] batch 13/13 in 3.79s\n",
      "Finished vit, time: 385.26s\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Feature Extraction -------------------\n",
    "for name, cnn_constructor, hf_config in extractor_configs:\n",
    "    print(f\"\\n--- Extracting features with {name} ---\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Instantiate model\n",
    "    if cnn_constructor:\n",
    "        model = cnn_constructor()\n",
    "        model.fc = torch.nn.Identity()\n",
    "        def extract_batch(imgs):\n",
    "            return model(imgs.to(device))\n",
    "    else:\n",
    "        proc_cls, model_cls, model_id = hf_config\n",
    "        processor = proc_cls(model_id)\n",
    "        model = model_cls(model_id).eval().to(device)\n",
    "        if name == 'clip':\n",
    "            def extract_batch(imgs):\n",
    "                pil = [transforms.ToPILImage()(img) for img in imgs]\n",
    "                inputs = processor(images=pil, return_tensors='pt', padding=True).to(device)\n",
    "                return model.get_image_features(**inputs)\n",
    "        else:  # vit\n",
    "            def extract_batch(imgs):\n",
    "                pil = [transforms.ToPILImage()(img) for img in imgs]\n",
    "                inputs = processor(images=pil, return_tensors='pt', padding=True).to(device)\n",
    "                out = model(**inputs)\n",
    "                return out.last_hidden_state[:,0,:]\n",
    "\n",
    "    # Loop splits\n",
    "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
    "        all_feats, all_labels = [], []\n",
    "        for i, (imgs, lbls) in enumerate(loader,1):\n",
    "            t0 = time.time()\n",
    "            with torch.no_grad(): feats = extract_batch(imgs)\n",
    "            feats_cpu = feats.cpu().numpy()\n",
    "            all_feats.append(feats_cpu)\n",
    "            all_labels.append(lbls.numpy())\n",
    "            print(f\"[{name}-{split}] batch {i}/{len(loader)} in {time.time()-t0:.2f}s\")\n",
    "        X = np.concatenate(all_feats)\n",
    "        y_arr = np.concatenate(all_labels)\n",
    "        np.save(os.path.join(FEATURES_DIR, f\"{name}_{split}_feats.npy\"), X)\n",
    "        np.save(os.path.join(FEATURES_DIR, f\"{name}_{split}_labels.npy\"), y_arr)\n",
    "        del all_feats, all_labels, X, y_arr\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Remove model\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Finished {name}, time: {time.time()-start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274b84fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "809d47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training classifiers on resnet50 features ---\n",
      "Shape of training data is (13232, 512)\n",
      "Training dt classifier...\n",
      "[resnet50-dt] acc=0.5154 precision=0.5245 recall=0.5154 f1=0.5195 time=19.30s\n",
      "Training logistic classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resnet50-logistic] acc=0.7104 precision=0.7168 recall=0.7104 f1=0.7119 time=0.59s\n",
      "Training nb classifier...\n",
      "[resnet50-nb] acc=0.5837 precision=0.6473 recall=0.5837 f1=0.5964 time=0.16s\n",
      "Training knn classifier...\n",
      "[resnet50-knn] acc=0.7536 precision=0.7989 recall=0.7536 f1=0.7563 time=0.10s\n",
      "Training softmax classifier...\n",
      "(13232, 512)\n",
      "Epoch 10, Loss: 1.3476, Accuracy: 0.6632\n",
      "Epoch 20, Loss: 1.0097, Accuracy: 0.7122\n",
      "Epoch 30, Loss: 0.8381, Accuracy: 0.7364\n",
      "Epoch 40, Loss: 0.7249, Accuracy: 0.7542\n",
      "Epoch 50, Loss: 0.6407, Accuracy: 0.7588\n",
      "Epoch 60, Loss: 0.5788, Accuracy: 0.7609\n",
      "[INFO] Early stopping at epoch 68\n",
      "[INFO] Best accuracy: 0.7633 at epoch 58\n",
      "[resnet50-softmax] acc=0.7633 precision=0.7598 recall=0.7627 f1=0.7601\n",
      "Training mlp classifier...\n",
      "(13232, 512)\n",
      "Epoch 10, Loss: 0.0643, Accuracy: 0.7993\n",
      "Epoch 20, Loss: 0.0127, Accuracy: 0.7950\n",
      "Epoch 30, Loss: 0.0053, Accuracy: 0.8011\n",
      "[INFO] Early stopping at epoch 34\n",
      "[INFO] Best accuracy: 0.8050 at epoch 24\n",
      "[resnet50-mlp] acc=0.8050 precision=0.8014 recall=0.8041 f1=0.7971\n",
      "\n",
      "--- Training classifiers on clip features ---\n",
      "Shape of training data is (13232, 512)\n",
      "Training dt classifier...\n",
      "[clip-dt] acc=0.3894 precision=0.4085 recall=0.3894 f1=0.3977 time=15.44s\n",
      "Training logistic classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clip-logistic] acc=0.5659 precision=0.5534 recall=0.5659 f1=0.5529 time=3.27s\n",
      "Training nb classifier...\n",
      "[clip-nb] acc=0.2113 precision=0.5532 recall=0.2113 f1=0.2036 time=0.13s\n",
      "Training knn classifier...\n",
      "[clip-knn] acc=0.5299 precision=0.5068 recall=0.5299 f1=0.5001 time=0.12s\n",
      "Training softmax classifier...\n",
      "(13232, 512)\n",
      "Epoch 10, Loss: 1.8765, Accuracy: 0.5163\n",
      "Epoch 20, Loss: 1.5658, Accuracy: 0.5517\n",
      "Epoch 30, Loss: 1.3510, Accuracy: 0.5862\n",
      "Epoch 40, Loss: 1.2154, Accuracy: 0.6010\n",
      "Epoch 50, Loss: 1.1339, Accuracy: 0.6103\n",
      "Epoch 60, Loss: 1.0849, Accuracy: 0.6137\n",
      "[INFO] Early stopping at epoch 69\n",
      "[INFO] Best accuracy: 0.6146 at epoch 59\n",
      "[clip-softmax] acc=0.6146 precision=0.5779 recall=0.6134 f1=0.5801\n",
      "Training mlp classifier...\n",
      "(13232, 512)\n",
      "Epoch 10, Loss: 0.7396, Accuracy: 0.6264\n",
      "[INFO] Early stopping at epoch 14\n",
      "[INFO] Best accuracy: 0.6333 at epoch 4\n",
      "[clip-mlp] acc=0.6333 precision=0.5852 recall=0.6155 f1=0.5862\n",
      "\n",
      "--- Training classifiers on vit features ---\n",
      "Shape of training data is (13232, 768)\n",
      "Training dt classifier...\n",
      "[vit-dt] acc=0.3519 precision=0.3577 recall=0.3519 f1=0.3546 time=24.39s\n",
      "Training logistic classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniforge3/envs/eeg_env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vit-logistic] acc=0.5003 precision=0.5096 recall=0.5003 f1=0.5014 time=165.12s\n",
      "Training nb classifier...\n",
      "[vit-nb] acc=0.2288 precision=0.4998 recall=0.2288 f1=0.2344 time=0.29s\n",
      "Training knn classifier...\n",
      "[vit-knn] acc=0.5082 precision=0.4725 recall=0.5082 f1=0.4770 time=0.14s\n",
      "Training softmax classifier...\n",
      "(13232, 768)\n",
      "Epoch 10, Loss: 1.7942, Accuracy: 0.5006\n",
      "Epoch 20, Loss: 1.4617, Accuracy: 0.5323\n",
      "Epoch 30, Loss: 1.2357, Accuracy: 0.5571\n",
      "Epoch 40, Loss: 1.0918, Accuracy: 0.5819\n",
      "Epoch 50, Loss: 0.9980, Accuracy: 0.5846\n",
      "Epoch 60, Loss: 0.9406, Accuracy: 0.5877\n",
      "[INFO] Early stopping at epoch 65\n",
      "[INFO] Best accuracy: 0.5892 at epoch 55\n",
      "[vit-softmax] acc=0.5892 precision=0.5395 recall=0.5798 f1=0.5499\n",
      "Training mlp classifier...\n",
      "(13232, 768)\n",
      "Epoch 10, Loss: 0.4012, Accuracy: 0.5985\n",
      "[INFO] Early stopping at epoch 14\n",
      "[INFO] Best accuracy: 0.6176 at epoch 4\n",
      "[vit-mlp] acc=0.6176 precision=0.5590 recall=0.5955 f1=0.5620\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Classifier Training -------------------\n",
    "from classifiers_eeg_embeds import SoftmaxClassifier, MLPClassifier, train_pytorch_model\n",
    "from sklearn.tree           import DecisionTreeClassifier\n",
    "from sklearn.linear_model  import LogisticRegression\n",
    "from sklearn.naive_bayes   import GaussianNB\n",
    "from sklearn.neighbors     import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics       import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import time\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, _, _ in extractor_configs:\n",
    "    print(f\"\\n--- Training classifiers on {name} features ---\")\n",
    "    X_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_feats.npy\"))\n",
    "    y_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_labels.npy\"))\n",
    "    X_val = np.load(os.path.join(FEATURES_DIR, f\"{name}_val_feats.npy\"))\n",
    "    y_val = np.load(os.path.join(FEATURES_DIR, f\"{name}_val_labels.npy\"))\n",
    "\n",
    "    # scale\n",
    "    scaler = StandardScaler()\n",
    "    X_tr = scaler.fit_transform(X_tr)\n",
    "    X_val = scaler.transform(X_val)\n",
    "\n",
    "    # optional PCA for resnet50\n",
    "    if name == \"resnet50\":\n",
    "        pca = PCA(n_components=512, whiten=True, random_state=42)\n",
    "        X_tr = pca.fit_transform(X_tr)\n",
    "        X_val = pca.transform(X_val)\n",
    "\n",
    "    num_classes = len(np.unique(y_tr))\n",
    "    input_dim   = X_tr.shape[1]\n",
    "    results[name] = {}\n",
    "\n",
    "    print(f\"Shape of training data is {X_tr.shape}\")\n",
    "    # --- scikit‑learn classifiers ---\n",
    "    for clf_name, clf in {\n",
    "        'dt'      : DecisionTreeClassifier(random_state=42),\n",
    "        'logistic': LogisticRegression(\n",
    "                         multi_class='multinomial',\n",
    "                         max_iter=1000,\n",
    "                         random_state=42\n",
    "                     ),\n",
    "        'nb'      : GaussianNB(),\n",
    "        'knn'     : KNeighborsClassifier(n_neighbors=5)\n",
    "    }.items():\n",
    "        print(f\"Training {clf_name} classifier...\")\n",
    "        t0 = time.time()\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        preds = clf.predict(X_val)\n",
    "        acc       = accuracy_score(y_val, preds)\n",
    "        prec      = precision_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        rec       = recall_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        f1        = f1_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        print(f\"[{name}-{clf_name}] acc={acc:.4f} precision={prec:.4f} \"\n",
    "              f\"recall={rec:.4f} f1={f1:.4f} time={time.time()-t0:.2f}s\")\n",
    "\n",
    "        results[name][clf_name] = {\n",
    "            'accuracy' : acc,\n",
    "            'precision': prec,\n",
    "            'recall'   : rec,\n",
    "            'f1'       : f1,\n",
    "            # 'report'   : classification_report(\n",
    "            #                  y_val, preds,\n",
    "            #                  target_names=unique_labels,\n",
    "            #                  zero_division=0\n",
    "            #              ),\n",
    "            'cm'       : confusion_matrix(y_val, preds)\n",
    "        }\n",
    "        del clf\n",
    "\n",
    "    # --- PyTorch classifiers ---\n",
    "    for clf_name, Model in {\n",
    "        'softmax': SoftmaxClassifier,\n",
    "        'mlp'    : lambda in_dim, n_cls: MLPClassifier(in_dim, [512], n_cls)\n",
    "    }.items():\n",
    "        print(f\"Training {clf_name} classifier...\")\n",
    "        # instantiate\n",
    "        if clf_name == 'softmax':\n",
    "            model = Model(input_dim, num_classes)\n",
    "        else:\n",
    "            model = Model(input_dim, num_classes)\n",
    "        pt = train_pytorch_model(\n",
    "            model, X_tr, X_val, y_tr, y_val,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            device=device\n",
    "        )\n",
    "        preds = pt['y_pred']\n",
    "        acc   = pt['accuracy']\n",
    "        prec  = precision_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        rec   = recall_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        f1_   = f1_score(y_val, preds, average='weighted', zero_division=0)\n",
    "        print(f\"[{name}-{clf_name}] acc={acc:.4f} precision={prec:.4f} \"\n",
    "              f\"recall={rec:.4f} f1={f1_:.4f}\")\n",
    "\n",
    "        results[name][clf_name] = {\n",
    "            'accuracy'      : acc,\n",
    "            'precision'     : prec,\n",
    "            'recall'        : rec,\n",
    "            'f1'            : f1_,\n",
    "            'train_losses'  : pt['train_losses'],\n",
    "            'val_accuracies': pt['val_accuracies'],\n",
    "            # 'report'        : classification_report(\n",
    "            #                       y_val, preds,\n",
    "            #                       target_names=unique_labels,\n",
    "            #                       zero_division=0\n",
    "            #                   ),\n",
    "            'cm'            : confusion_matrix(y_val, preds)\n",
    "        }\n",
    "        del model, pt\n",
    "\n",
    "    # free memory before next extractor\n",
    "    del X_tr, y_tr, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f836e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "with open(RESULTS_PATH, 'wb') as f:\n",
    "    pickle.dump(results, f)\n",
    "print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f78907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ------------------- Classifier Training -------------------\n",
    "# classifiers = {\n",
    "#     'lr': LogisticRegression(max_iter=1000),\n",
    "#     'rf': RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42),\n",
    "#     # 'gb': GradientBoostingClassifier(),\n",
    "#     'xgb': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', n_jobs=-1, random_state=42),\n",
    "#     'mlp': MLPClassifier(hidden_layer_sizes=(512,), max_iter=200),\n",
    "#     'knn': KNeighborsClassifier(n_neighbors=5)\n",
    "# }\n",
    "# results = {}\n",
    "\n",
    "# for name, _, _ in extractor_configs:\n",
    "#     print(f\"\\n--- Training classifiers on {name} features ---\")\n",
    "#     X_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_feats.npy\"))\n",
    "#     y_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_labels.npy\"))\n",
    "#     X_val = np.load(os.path.join(FEATURES_DIR, f\"{name}_val_feats.npy\"))\n",
    "#     y_val = np.load(os.path.join(FEATURES_DIR, f\"{name}_val_labels.npy\"))\n",
    "#     # add standard scaler\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "#     scaler = StandardScaler()\n",
    "#     X_tr = scaler.fit_transform(X_tr)\n",
    "#     X_val = scaler.transform(X_val)\n",
    "#     if name == \"resnet50\":\n",
    "#         # reduce dimensionality for resnet50 features\n",
    "#         from sklearn.decomposition import PCA\n",
    "#         pca = PCA(n_components=512, whiten=True, random_state=42)\n",
    "#         pca.fit(X_tr)\n",
    "#         X_tr = pca.transform(X_tr)\n",
    "#         X_val = pca.transform(X_val)\n",
    "#     results[name] = {}\n",
    "#     for clf_name, clf in classifiers.items():\n",
    "#         # add debug information\n",
    "#         print(f\"Training {clf_name} classifier...\")\n",
    "#         print(f\"X_tr shape: {X_tr.shape}, y_tr shape: {y_tr.shape}\")\n",
    "#         t0 = time.time()\n",
    "#         clf.fit(X_tr, y_tr)\n",
    "#         preds = clf.predict(X_val)\n",
    "#         acc = accuracy_score(y_val, preds)\n",
    "#         # calculate precision, recall, f1-score\n",
    "#         precision = precision_score(y_val, preds, average='weighted', zero_division=0)\n",
    "#         recall = recall_score(y_val, preds, average='weighted', zero_division=0)\n",
    "#         f1 = f1_score(y_val, preds, average='weighted', zero_division=0)\n",
    "#         print(f\"[{name}-{clf_name}] acc={acc:.4f} precision={precision:.4f} recall={recall:.4f} f1={f1:.4f} time={time.time()-t0:.2f}s\")\n",
    "#         results[name][clf_name] = {\n",
    "#             'accuracy': acc,\n",
    "#             'precision': precision,\n",
    "#             'recall': recall,\n",
    "#             'f1': f1,\n",
    "#             'report': classification_report(y_val, preds, target_names=unique_labels, zero_division=0)\n",
    "#         }\n",
    "#         # print classification report\n",
    "#         # print(results[name][clf_name]['report'])\n",
    "#         del clf\n",
    "#     del X_tr, y_tr, X_val, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb0fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save results\n",
    "# with open(RESULTS_PATH, 'wb') as f:\n",
    "#     pickle.dump(results, f)\n",
    "# print(\"\\nAll done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cluster and visualize, using t-SNE, the Resnet50, CLIP and ViT features\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.decomposition import PCA\n",
    "# import seaborn as sns\n",
    "# import umap\n",
    "# import umap.umap_ as umap\n",
    "# import matplotlib.cm as cm\n",
    "# import matplotlib.colors as mcolors\n",
    "# import matplotlib.patches as mpatches\n",
    "# import matplotlib.lines as mlines\n",
    "\n",
    "# def plot_tsne(X, y, title, save_path):\n",
    "#     tsne = TSNE(n_components=2, random_state=42, n_jobs=-1, verbose=1)\n",
    "#     X_embedded = tsne.fit_transform(X)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='tab10', alpha=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter, ticks=range(len(unique_labels)), label='Concepts')\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_umap(X, y, title, save_path):\n",
    "#     reducer = umap.UMAP(n_components=2, random_state=42, n_jobs=-1, verbose=1)\n",
    "#     X_embedded = reducer.fit_transform(X)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='tab10', alpha=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter, ticks=range(len(unique_labels)), label='Concepts')\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# def plot_pca(X, y, title, save_path):\n",
    "#     pca = PCA(n_components=2, random_state=42, whiten=True)\n",
    "#     X_embedded = pca.fit_transform(X)\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, cmap='tab10', alpha=0.5)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar(scatter, ticks=range(len(unique_labels)), label='Concepts')\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "# # Plot and save t-SNE, UMAP and PCA for each feature set\n",
    "# for name, _, _ in extractor_configs:\n",
    "#     print(f\"\\n--- Plotting {name} features ---\")\n",
    "#     X_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_feats.npy\"))\n",
    "#     y_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_labels.npy\"))\n",
    "#     plot_tsne(X_tr, y_tr, f\"{name} t-SNE\", os.path.join(FEATURES_DIR, f\"{name}_train_tsne.png\"))\n",
    "#     plot_umap(X_tr, y_tr, f\"{name} UMAP\", os.path.join(FEATURES_DIR, f\"{name}_train_umap.png\"))\n",
    "#     plot_pca(X_tr, y_tr, f\"{name} PCA\", os.path.join(FEATURES_DIR, f\"{name}_train_pca.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1950b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now, run the visualizations again, but just for 10 classes, to make it easier to visualize\n",
    "# # Select 10 cllasses randomly from the unique labels\n",
    "# import random\n",
    "# random.seed(42)\n",
    "\n",
    "# num_classes = 50\n",
    "\n",
    "# # load the features and labels for each feature set and then compute the unique labels, and select 10 classes randomly, and plot the t-SNE, UMAP and PCA for each feature set\n",
    "# for name, _, _ in extractor_configs:\n",
    "#     print(f\"\\n--- Plotting {name} features for {num_classes} classes ---\")\n",
    "#     X_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_feats.npy\"))\n",
    "#     y_tr = np.load(os.path.join(FEATURES_DIR, f\"{name}_train_labels.npy\"))\n",
    "#     unique_labels = np.unique(y_tr)\n",
    "#     selected_labels = random.sample(list(unique_labels), num_classes)\n",
    "#     mask = np.isin(y_tr, selected_labels)\n",
    "#     X_tr = X_tr[mask]\n",
    "#     y_tr = y_tr[mask]\n",
    "#     plot_tsne(X_tr, y_tr, f\"{name} t-SNE ({num_classes} classes)\", os.path.join(FEATURES_DIR, f\"{name}_train_tsne_{num_classes}_classes.png\"))\n",
    "#     plot_umap(X_tr, y_tr, f\"{name} UMAP ({num_classes} classes)\", os.path.join(FEATURES_DIR, f\"{name}_train_umap_{num_classes}_classes.png\"))\n",
    "#     plot_pca(X_tr, y_tr, f\"{name} PCA ({num_classes} classes)\", os.path.join(FEATURES_DIR, f\"{name}_train_pca_{num_classes}_classes.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde74505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
