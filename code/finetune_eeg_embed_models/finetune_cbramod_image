#!/usr/bin/env python3
"""
fine_tune_cbramod_physio_style.py

Fine‑tune CBraMod with the Physio‑style classifier head, adapted to 17‑channel,
200‑sample (1 s) EEG-image trials. Supports 'stack' or 'average' subject handling.

Usage:
  python fine_tune_cbramod_physio_style.py \
    --eeg_root /path/to/eeg/preprocessed_data \
    --img_meta  /path/to/image_metadata.npy \
    --things_map /path/to/things_map.tsv \
    --subject_handling stack \
    --use_pretrained_weights \
    --foundation_dir /path/to/pretrained_weights.pth \
    --batch_size 32 \
    --lr 1e-4 \
    --epochs 25
"""
import os
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split

from cbramod import CBraMod                                             # :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}
from pre_process_eeg_cbramod import process_eeg                           # :contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}
from classifiers_eeg_embeds import load_image_labels

# -----------------------------------------------------------------------------
# Physio‑style Model head, adapted for image EEG (17 chans × 1 patch × 200 pts)
# -----------------------------------------------------------------------------
class Model(nn.Module):
    def __init__(self, param):
        super(Model, self).__init__()
        # CBraMod backbone: in_dim = #channels (17), out_dim/d_model = 200
        self.backbone = CBraMod(
            in_dim=200, out_dim=200, d_model=200,
            dim_feedforward=800,
            seq_len=30,
            n_layer=12, nhead=8
        )
        if param.use_pretrained_weights:
            map_location = torch.device(f'cuda:{param.cuda}' if torch.cuda.is_available() else 'cpu')
            self.backbone.load_state_dict(torch.load(param.foundation_dir, map_location=map_location))
        # bypass the default projection to out_dim
        self.backbone.proj_out = nn.Identity()

        flat_dim   = param.channels * param.patches * param.d_model  # 17×1×200 = 3400
        hidden_dim = param.patches * param.d_model                  # 1×200 = 200

        self.classifier = nn.Sequential(
            nn.Linear(flat_dim,   hidden_dim),
            nn.ELU(),
            nn.Dropout(param.dropout),
            nn.Linear(hidden_dim, param.d_model),
            nn.ELU(),
            nn.Dropout(param.dropout),
            nn.Linear(param.d_model, param.num_of_classes)
        )

    def forward(self, x):
        # x: [B, channels, patches, patch_size] = [B,17,1,200]
        feats = self.backbone(x)               # [B,17,1,200]
        B, C, P, D = feats.shape
        out = feats.contiguous().view(B, C*P*D)
        return self.classifier(out)

# -----------------------------------------------------------------------------
# Dataset: exactly as before, but keep channels=17, patch_size=200
# -----------------------------------------------------------------------------
class EEGImageDataset(Dataset):
    def __init__(self, eeg_root, img_meta_path, things_map_path, subject_handling):
        self.handling = subject_handling
        # load THINGS metadata
        img_meta      = np.load(img_meta_path, allow_pickle=True).item()
        concepts      = img_meta['train_img_concepts']
        files         = img_meta['train_img_files']
        label_map     = load_image_labels(img_meta_path, things_map_path)

        raws, labels = [], []
        per_image = {i: [] for i in range(len(concepts))}

        for subj in sorted(os.listdir(eeg_root)):
            subj_fp = os.path.join(eeg_root, subj, 'preprocessed_eeg_training.npy')
            if not os.path.exists(subj_fp): continue
            data = np.load(subj_fp, allow_pickle=True).item()
            eeg  = data['preprocessed_eeg_data']    # [n_cond, n_rep, 17, 200]
            n_cond, n_rep, _, _ = eeg.shape

            if self.handling == 'stack':
                for i in range(n_cond):
                    key = f"{concepts[i]}/{files[i]}".lower()
                    lbl = label_map[key]
                    for r in range(n_rep):
                        raws.append(eeg[i, r])         # (17,200)
                        labels.append(lbl)
            else:
                subj_mean = eeg.mean(axis=1)         # [n_cond,17,200]
                for i in range(n_cond):
                    per_image[i].append(subj_mean[i])

        if self.handling == 'average':
            for i, mats in per_image.items():
                raws.append(np.stack(mats,0).mean(0))
                key = f"{concepts[i]}/{files[i]}".lower()
                labels.append(label_map[key])

        # build label→idx
        uniq = sorted(set(labels))
        self.label2idx = {l:i for i,l in enumerate(uniq)}
        self.num_of_classes = len(uniq)
        self.raws   = raws
        self.labels = [self.label2idx[l] for l in labels]

    def __len__(self): return len(self.raws)
    def __getitem__(self, idx):
        raw = self.raws[idx].T                            # (200,17)
        proc= process_eeg(raw,       orig_sfreq=200)      # → [1,17,patches=1,200] :contentReference[oaicite:4]{index=4}:contentReference[oaicite:5]{index=5}
        x   = torch.from_numpy(proc).float().squeeze(0)   # [17,1,200]
        y   = self.labels[idx]
        return x, y

# -----------------------------------------------------------------------------
# Training / evaluation routines (standard PyTorch)
# -----------------------------------------------------------------------------
def train_epoch(m,o,crit,loader,dev):
    m.train(); total,loss=0,0
    for x,y in loader:
        x,y = x.to(dev), y.to(dev)
        o.zero_grad()
        l = crit(m(x), y)
        l.backward(); o.step()
        loss += l.item()*x.size(0); total += x.size(0)
    return loss/total

@torch.no_grad()
def eval_epoch(m,crit,loader,dev):
    m.eval(); total,loss,correct=0,0,0
    for x,y in loader:
        x,y = x.to(dev), y.to(dev)
        logits = m(x); loss += crit(logits,y).item()*x.size(0)
        pred = logits.argmax(1); correct += (pred==y).sum().item()
        total += x.size(0)
    return loss/total, correct/total

# -----------------------------------------------------------------------------
# Main CLI
# -----------------------------------------------------------------------------
if __name__=="__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--eeg_root",            required=True)
    parser.add_argument("--img_meta",            required=True)
    parser.add_argument("--things_map",          required=True)
    parser.add_argument("--subject_handling",    choices=["stack","average"], default="stack")
    parser.add_argument("--use_pretrained_weights", action="store_true")
    parser.add_argument("--foundation_dir",      default="pretrained_weights.pth")
    parser.add_argument("--batch_size",    type=int,   default=32)
    parser.add_argument("--lr",            type=float, default=1e-4)
    parser.add_argument("--epochs",        type=int,   default=25)
    parser.add_argument("--dropout",       type=float, default=0.2)
    parser.add_argument("--cuda",          type=int,   default=0)
    args = parser.parse_args()

    # augment args with model params
    args.channels        = 17
    args.d_model         = 200
    args.dim_feedforward = 800
    args.seq_len         = 1
    args.patches         = 30
    args.n_layer         = 12
    args.nhead           = 8

    # dataset & split
    ds = EEGImageDataset(args.eeg_root, args.img_meta, args.things_map, args.subject_handling)
    n_val = int(len(ds)*0.2); n_tr = len(ds)-n_val
    tr,va = random_split(ds,[n_tr,n_val])
    dl_tr = DataLoader(tr, batch_size=args.batch_size, shuffle=True)
    dl_va = DataLoader(va, batch_size=args.batch_size)

    device = torch.device(f"cuda:{args.cuda}" if torch.cuda.is_available() else "cpu")
    model = Model(args).to(device)
    opt   = optim.Adam(model.parameters(), lr=args.lr)
    crit  = nn.CrossEntropyLoss()

    best_acc=0
    for ep in range(1, args.epochs+1):
        tr_loss = train_epoch(model,opt,crit,dl_tr,device)
        va_loss, va_acc = eval_epoch(model,crit,dl_va,device)
        print(f"Ep{ep}  tr_loss:{tr_loss:.4f}  va_loss:{va_loss:.4f}  va_acc:{va_acc:.2%}")
        if va_acc>best_acc:
            best_acc=va_acc
            torch.save(model.state_dict(),"best_cbramod_image_model.pth")
    print(f"Done. Best val acc: {best_acc:.2%}")
